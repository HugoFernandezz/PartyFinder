name: Scrape Events

on:
  # Ejecutar 3 veces al día: 10:00, 16:00, 22:00 (hora España)
  # España es UTC+1 en invierno, UTC+2 en verano
  schedule:
    - cron: '0 9,15,21 * * *'   # UTC -> 10:00, 16:00, 22:00 España (invierno)
  
  # Permitir ejecución manual
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: backend/requirements.txt
      
      - name: Install dependencies
        run: |
          cd backend
          pip install -r requirements.txt
      
      - name: Install Playwright Chromium
        run: |
          pip install playwright
          playwright install chromium
          playwright install-deps chromium
      
      - name: Create data directory
        run: mkdir -p backend/data
      
      - name: Run scraper with Firebase upload
        env:
          FIREBASE_SERVICE_ACCOUNT: ${{ secrets.FIREBASE_SERVICE_ACCOUNT }}
        run: |
          cd backend
          python scraper.py --firebase
      
      - name: Upload artifacts (backup)
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: scraped-data-${{ github.run_number }}
          path: backend/data/*.json
          retention-days: 7
  notify-on-failure:
    runs-on: ubuntu-latest
    needs: scrape
    if: needs.scrape.result == 'failure'
    
    steps:
      - name: Log failure
        run: |
          echo "Scraping failed at $(date)"
          echo "Check the workflow logs for details"


