name: Scrape Events

on:
  # Ejecutar todos los días a las 13:00 hora española (12:00 UTC en invierno)
  schedule:
    - cron: '0 12 * * *'
  
  # Permitir ejecución manual
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: backend/requirements.txt
      
      - name: Install dependencies
        run: |
          cd backend
          pip install -r requirements.txt
      
      - name: Install Firecrawl SDK
        run: pip install firecrawl-py
      
      - name: Create data directory
        run: mkdir -p backend/data

      - name: Create Firebase Credentials
        env:
          FIREBASE_KEY: ${{ secrets.FIREBASE_SERVICE_ACCOUNT }}
        run: |
          cd backend
          echo "$FIREBASE_KEY" > serviceAccountKey.json
      
      - name: Run Firecrawl scraper with Firebase upload
        env:
          FIRECRAWL_API_KEY: ${{ secrets.FIRECRAWL_API_KEY }}
        run: |
          cd backend
          python scraper_firecrawl.py --upload
      
      - name: Upload artifacts (backup)
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: scraped-data-${{ github.run_number }}
          path: backend/data/*.json
          retention-days: 7
  notify-on-failure:
    runs-on: ubuntu-latest
    needs: scrape
    if: needs.scrape.result == 'failure'
    
    steps:
      - name: Log failure
        run: |
          echo "Scraping failed at $(date)"
          echo "Check the workflow logs for details"


